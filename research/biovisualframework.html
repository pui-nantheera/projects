<!DOCTYPE html>
<html>
    <head>
        <link type="text/css" rel="stylesheet" href="..\stylesheet.css"/>
        <link href='http://fonts.googleapis.com/css?family=Roboto+Slab:400,700' rel='stylesheet' type='text/css'>
        <link href='http://fonts.googleapis.com/css?family=Merriweather+Sans:400,700' rel='stylesheet' type='text/css'>
        <title>Bioinspired</title>
        <meta name="description" content="Bio-inspired visual framework for biped robotics">
        <meta charset="utf-8">
    </head>
    <body>
        <div id="header">
            <p id="name">Nantheera Anantrasirichai</p>
        </div>
        <div class="left">
            <nav class="textmenu">
                <ul>
                  <li><a href="..\index.html">Home</a></li>
                  <li><a href="..\about.html">About Pui</a></li>
                  <li><a href="..\research.html">Research</a></li>
                  <li><a href="..\publications.html">Publications</a></li>
                  <li><a href="..\download.html">Download</a></li>
                  <li><a href="..\links.html">Links</a></li>
                </ul>
            </nav>
        </div>
        <div class="body">
            <h2>Bio-inspired visual framework for autonomous locomotion</h2>
                <div class="researchlist">
                    <p>Vision provides us information that can be used for adaptively controlling our locomotion. However,
                        we still do not fully understand how humans perceive and use it in a dynamic environment.
                        This implies that information from visual sensors, e.g. cameras, has not yet been
                        fully employed in autonomous systems. This project will study human eye movement during
                        locomotion using a mobile eye tracker, leading to a better understanding of human perception
                        and what low-level features drive decisions, particularly in complex visual task. Recent
                        research suggested that reflexive (intense stimuli) and controlled (voluntary) processes converge
                        on a common neural architecture. Therefore, we are developing bottom-up and top-down
                        approaches together, i.e. saliency for locomotion will be modelled associated to the task difficulty. </p>

                    <h4>Visual Salience and Priority Estimation for Locomotion</h4>

                    <p>We proposed a novel method of salience and priority
                        estimation for the human visual system during locomotion.
                        This visual information contains dynamic content derived
                        from a moving viewpoint. The priority map, ranking
                        key areas on the image, is created from probabilities of gaze
                        fixations, merged from bottom-up features and top-down control
                        on the locomotion. Two deep convolutional neural networks
                        (CNNs), inspired by models of the primate visual system,
                        are employed to capture local salience features and compute
                        probabilities. The first network operates through the
                        foveal and peripheral areas around the eye positions. The
                        second network obtains the importance of fixated points that
                        have long durations or multiple visits, of which such areas
                        need more times to process or to recheck to ensure smooth
                        locomotion. The results show that our proposed method outperforms
                        the state-of-the-art by up to 30%, computed from
                        average of four well known metrics for saliency estimation.</p>


                    <p><img class="figuremiddle" src="biovisualframework/prioritymap.png" alt="Proposed process" height="256"></p>

                    <h4>Fixation Identification for Low-Sample-Rate Mobile Eye Trackers</h4>

                    <p>We proposed a novel method of fixation identification
                        for mobile eye trackers. The most significant benefit of our
                        method over the state-of-the-art is that it achieves high accuracy
                        for low-sample-rate devices worn during locomotion.
                        This in turn delivers higher quality datasets for further use in
                        human behaviour research, robotics and the development of
                        guidance aids for the visually impaired. The proposed method
                        employs temporal characteristics of the eye positions combined
                        with statistical visual features extracted using a deep
                        convolutional neural network through the fovea and peripheral areas
                        around the eye positions. The results show that the proposed
                        method outperforms existing methods by up to 16 % in terms
                        of classification accuracy.</p>

                    <h4>References</h4>
                    <ul>
                        <li><span class="papername">Fixation Prediction and Visual Priority Maps for Biped Locomotion</span>. N. Anantrasirichai, K. A. J. Daniels, J. F. Burn, Iain D. Gilchrist and David Bull. IEEE Transactions on Cybernetics. 2017. <pdf>[</pdf><a href="..\papers\fixation_CYB.pdf">PDF</a><pdf>]</pdf> <pdf>[</pdf><a href="..\download.html#eyetrackingseq">Eye tracking data</a><pdf>]</pdf></li>
                        <li><span class="papername">Fixation Identification for Low-Sample-Rate Mobile Eye Trackers</span>. N. Anantrasirichai, I. D. Gilchrist  and David Bull. In Proceedings of the IEEE International Conference on Image Processing (ICIP 2016). <pdf>[</pdf><a href="..\papers\event_detection.pdf">PDF</a><pdf>]</pdf> </li>
                        <li><span class="papername">Visual Salience and Priority Estimation for Locomotion using a Deep Convolutional Neural Network</span>. N. Anantrasirichai, I. D. Gilchrist  and David Bull. In Proceedings of the IEEE International Conference on Image Processing (ICIP 2016). <pdf>[</pdf><a href="..\papers\Anantrasirichai_visual_final.pdf">PDF</a><pdf>]</pdf></li>

                    </ul>
                </div>
        </div>
        <div id="footer">
            <p>Office: room 2.34 Merchant Venturer's Building, Bristol, UK, BS8 1UB | Tel: +44 (0) 117-331-5075</p>
        </div>
    </body>
    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-41219445-1', 'bris.ac.uk');
        ga('send', 'pageview');
    </script>
</html>
